{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How does unsqueeze help us to solve certain broadcasting problems?**\n",
    "\n",
    "The unsqueeze function in PyTorch is used to add dimensions to a tensor at specified positions. It can help solve certain broadcasting problems by adjusting the shape of tensors to make them compatible for elementwise operations.\n",
    "\n",
    "By using unsqueeze, you can explicitly add dimensions to a tensor, which can align its shape with another tensor for broadcasting. It is particularly useful when the tensors have different numbers of dimensions, and you want to match their shapes to perform elementwise operations.\n",
    "\n",
    "Here's an example to illustrate how unsqueeze can solve broadcasting problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 22, 33]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)\n",
    "B = torch.tensor([10, 20, 30])  # Shape: (3,)\n",
    "\n",
    "B_unsqueezed = B.unsqueeze(0)  # Add a dimension at position 0\n",
    "\n",
    "C = A + B_unsqueezed\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have tensors A and B with different shapes. Tensor A has shape (1, 3), and tensor B has shape (3,). To perform elementwise addition between A and B, we need to ensure their shapes are compatible.\n",
    "\n",
    "The unsqueeze method is used to add a dimension to tensor B at position 0, effectively changing its shape from (3,) to (1, 3). The resulting tensor, B_unsqueezed, now has the same number of dimensions as A, allowing us to perform elementwise addition without broadcasting errors.\n",
    "\n",
    "By using unsqueeze, we explicitly adjust the shape of tensors to make them compatible for elementwise operations. It provides fine-grained control over the dimensions of tensors and can help align their shapes, enabling broadcasting between tensors with different dimensionalities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How can we use indexing to do the same operation as unsqueeze?**\n",
    "\n",
    "In PyTorch, indexing can be used to achieve the same operation as unsqueeze by manipulating the dimensions of a tensor. By selecting specific indices and using the None keyword, you can effectively add dimensions to a tensor.\n",
    "\n",
    "Here's an example of how indexing can be used to achieve the same result as unsqueeze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 22, 33]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "A = torch.tensor([[1, 2, 3]])  # Shape: (1, 3)\n",
    "B = torch.tensor([10, 20, 30])  # Shape: (3,)\n",
    "\n",
    "B_indexed = B[None, :]  # Add a dimension at position 0\n",
    "\n",
    "C = A + B_indexed\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use indexing to add a dimension to tensor B at position 0. The index None is used to create a new dimension along that axis. By indexing B with [None, :], we effectively reshape it from (3,) to (1, 3), matching the shape of tensor A.\n",
    "\n",
    "The resulting tensor, B_indexed, has the same shape as A, allowing us to perform elementwise addition without broadcasting errors.\n",
    "\n",
    "By using indexing with the None keyword, you can manipulate the dimensions of tensors and add new dimensions where needed. This technique provides an alternative way to achieve the same effect as unsqueeze and can be useful when you want to control the tensor's shape during broadcasting operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. How do we show the actual contents of the memory used for a tensor?**\n",
    "\n",
    "To show the actual contents of the memory used for a tensor in PyTorch, you can use the .numpy() or .tolist() method to access the underlying data and convert it to a NumPy array or a Python list, respectively. This allows you to inspect the values stored in the memory of the tensor.\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [1 2 3 4 5]\n",
      "Python list: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "array = x.numpy()\n",
    "\n",
    "# Convert the tensor to a Python list\n",
    "lst = x.tolist()\n",
    "\n",
    "print(\"NumPy array:\", array)\n",
    "print(\"Python list:\", lst)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a tensor x with values [1, 2, 3, 4, 5]. To access the actual contents of the memory used by the tensor, we use .numpy() to convert it to a NumPy array, and .tolist() to convert it to a Python list.\n",
    "\n",
    "By printing the NumPy array or the Python list, you can see the actual values stored in the memory of the tensor.\n",
    "\n",
    "Note that accessing the underlying memory of a tensor using .numpy() or .tolist() may involve data copying if the tensor is stored on the GPU. In such cases, you may need to first move the tensor to the CPU using .cpu() before converting it to a NumPy array or a Python list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)**\n",
    "\n",
    "When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each column of the matrix. This is known as column-wise addition or broadcasting.\n",
    "\n",
    "Here's an example to demonstrate this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  4,  6],\n",
      "        [ 5,  7,  9],\n",
      "        [ 8, 10, 12]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "result = matrix + vector\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a vector vector of size 3 and a matrix matrix of size 3x3. When we add the vector to the matrix using the + operator, the elements of the vector are added to each column of the matrix. Each element in the vector is added to the corresponding column in the matrix.\n",
    "\n",
    "As a result, the resulting tensor result has the same shape as the matrix, and each element in result is the sum of the corresponding element in the matrix and the corresponding element in the vector.\n",
    "\n",
    "Note that if you want to perform row-wise addition instead, where the elements of the vector are added to each row of the matrix, you can use the transpose operation on the matrix before performing the addition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Do broadcasting and expand_as result in increased memory use? Why or why not?**\n",
    "\n",
    "Broadcasting and `expand_as` operations in PyTorch do not result in increased memory usage. Instead, they allow you to perform operations efficiently without explicitly creating new tensors or consuming additional memory.\n",
    "\n",
    "Here's why broadcasting and `expand_as` operations do not increase memory usage:\n",
    "\n",
    "1. Broadcasting: Broadcasting allows for elementwise operations between tensors of different shapes by implicitly replicating values along appropriate dimensions. The broadcasting mechanism avoids the need to create multiple copies of the data in memory. Instead, it uses a concept called \"virtual\" or \"lazy\" evaluation, where the operation is performed on-the-fly without explicitly allocating new memory for the expanded tensors. The result is a view into the original tensors, with no additional memory consumption.\n",
    "\n",
    "2. `expand_as`: The `expand_as` method is used to expand the dimensions of a tensor to match the shape of another tensor. It returns a new tensor that shares the same data as the original tensor but with expanded dimensions. Similar to broadcasting, `expand_as` does not allocate new memory for the expanded tensor. Instead, it creates a view into the original tensor's data, ensuring that memory consumption remains unchanged.\n",
    "\n",
    "Both broadcasting and `expand_as` provide a way to perform operations efficiently without duplicating or increasing memory usage. They leverage the underlying tensor's data and shape to perform computations effectively, avoiding unnecessary memory allocation.\n",
    "\n",
    "It's worth noting that if you perform an in-place operation on a tensor resulting from broadcasting or `expand_as`, it may lead to increased memory usage, as the original tensor's data may need to be copied to accommodate the changes. However, the broadcasting and `expand_as` operations themselves do not directly cause increased memory usage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Implement matmul using Einstein summation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 58,  64],\n",
      "        [139, 154]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two matrices\n",
    "A = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "B = torch.tensor([[7, 8],\n",
    "                  [9, 10],\n",
    "                  [11, 12]])\n",
    "\n",
    "# Perform matrix multiplication using Einstein summation\n",
    "C = torch.einsum('ij, jk -> ik', A, B)\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have two matrices A and B. To perform matrix multiplication using Einstein summation, we use the torch.einsum function and specify the Einstein summation notation string 'ij, jk -> ik'.\n",
    "\n",
    "Here, 'ij' represents the dimensions of matrix A, 'jk' represents the dimensions of matrix B, and 'ik' represents the dimensions of the resulting matrix C. The arrow -> indicates the output dimensions.\n",
    "\n",
    "The resulting tensor C is obtained by performing matrix multiplication using Einstein summation. The notation 'ij, jk -> ik' specifies that the index j is contracted or summed over, resulting in the desired matrix multiplication operation.\n",
    "\n",
    "Einstein summation provides a concise and powerful way to express matrix operations, including matmul, by specifying the index contraction pattern. It allows for a more compact representation of complex tensor operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What does a repeated index letter represent on the lefthand side of einsum?**\n",
    "\n",
    "\n",
    "In Einstein summation notation, a repeated index letter on the left-hand side of `einsum` represents summation or contraction along that index. It indicates that the values along the repeated index are summed over in the resulting output.\n",
    "\n",
    "For example, let's consider the notation `'ij, j -> i'` in `einsum`. Here, the index letter `'j'` appears on both the left-hand side and the right-hand side of the `->` arrow. This indicates that the values along the `'j'` index in the input tensors are summed over, resulting in a contraction along that index.\n",
    "\n",
    "Here's a code example to illustrate this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create input tensors\n",
    "A = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "B = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Perform Einstein summation with repeated index 'j'\n",
    "C = torch.einsum('ij, j -> i', A, B)\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a matrix `A` and a vector `B`. The Einstein summation notation `'ij, j -> i'` specifies that the index `'j'` is repeated, indicating summation along that index. The resulting tensor `C` is obtained by performing elementwise multiplication between the corresponding elements of `A` and `B` and then summing over the repeated index `'j'`.\n",
    "\n",
    "The repeated index letter on the left-hand side of `einsum` indicates that the corresponding dimensions are contracted or summed over, resulting in the desired output shape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. What are the three rules of Einstein summation notation? Why?**\n",
    "\n",
    "The three rules of Einstein summation notation, also known as the Einstein summation convention, are as follows:\n",
    "\n",
    "1. Repeated Index Rule: When an index appears twice in a term, it implies summation over that index. The repeated index is summed over all possible values within its valid range. This rule simplifies the notation by eliminating the need for explicit summation symbols, such as the sigma (∑) symbol.\n",
    "\n",
    "2. Index Naming Rule: Each index can only appear twice in an expression—one as a subscript and one as a superscript. The subscripts indicate the indices of the input tensors, while the superscripts indicate the indices of the resulting output tensor.\n",
    "\n",
    "3. Index Equality Rule: If an index does not appear in the output, it is assumed to be summed over implicitly. This rule implies that the resulting output tensor has dimensions corresponding to the remaining indices that are not explicitly summed over.\n",
    "\n",
    "These rules of Einstein summation notation help simplify and compactly represent tensor operations, making them more concise and readable. By eliminating the need for explicit summation symbols and simplifying index notation, the notation becomes more expressive and easier to understand.\n",
    "\n",
    "The Einstein summation convention is based on the concept of tensor contraction, which involves multiplying corresponding elements of tensors and summing over the repeated indices. This convention provides a compact and efficient way to express complex tensor operations, such as matrix multiplication, dot products, and more, without explicitly specifying all the intermediate steps and summation symbols."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. What are the forward pass and backward pass of a neural network?**\n",
    "\n",
    "The forward pass and backward pass are two fundamental steps in the training and inference process of a neural network. They are essential for calculating the output of the network and updating the network's parameters during training. Here's an overview of each step:\n",
    "\n",
    "1. Forward Pass:\n",
    "   - During the forward pass, the input data is propagated through the network from the input layer to the output layer.\n",
    "   - Each layer performs a series of computations, including weighted sum and activation function, to generate an output.\n",
    "   - The output of one layer becomes the input to the next layer, and this process continues until the output layer is reached.\n",
    "   - The forward pass calculates the predicted output of the network based on the current values of the weights and biases.\n",
    "\n",
    "2. Backward Pass (Backpropagation):\n",
    "   - After the forward pass, the network compares the predicted output with the desired output to determine the loss or error.\n",
    "   - The backward pass, also known as backpropagation, calculates the gradients of the loss with respect to the weights and biases of the network.\n",
    "   - The gradients indicate the direction and magnitude of the parameter updates needed to minimize the loss.\n",
    "   - The backpropagation algorithm propagates the gradients from the output layer to the input layer, updating the weights and biases of each layer along the way.\n",
    "   - This process uses the chain rule of calculus to efficiently calculate the gradients by iteratively applying the derivatives of the activation functions and the dot product of the incoming gradients.\n",
    "   - The backward pass updates the parameters of the network, allowing it to learn from the training data and improve its performance.\n",
    "\n",
    "The forward pass computes the output of the neural network given the input, while the backward pass calculates the gradients that enable the network to adjust its parameters during training. The combination of these two steps forms the basis of training neural networks using gradient-based optimization algorithms, such as stochastic gradient descent (SGD) or its variants."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?**\n",
    "\n",
    "Storing some of the activations calculated for intermediate layers during the forward pass is necessary for the backward pass, specifically for the backpropagation algorithm. The backpropagation algorithm requires the intermediate activations to compute the gradients of the loss function with respect to the weights and biases of the network.\n",
    "\n",
    "Here are a few reasons why storing intermediate activations is important:\n",
    "\n",
    "1. Gradient Calculation: The backpropagation algorithm relies on the chain rule of calculus to calculate the gradients. To compute the gradients for each layer, the algorithm needs access to the activations of the previous layer. By storing the intermediate activations during the forward pass, the network can efficiently propagate the gradients backward through the layers.\n",
    "\n",
    "2. Parameter Update: During the backward pass, the gradients calculated with respect to the weights and biases are used to update these parameters. To update the parameters of a specific layer, the algorithm needs both the gradients and the corresponding activations of that layer.\n",
    "\n",
    "3. Memory Efficiency: Storing the intermediate activations allows the network to avoid redundant calculations during the backward pass. Without storing the activations, the network would need to recalculate them during backpropagation, which can be computationally expensive and inefficient.\n",
    "\n",
    "4. Deep Networks: In deep neural networks with many layers, storing intermediate activations becomes particularly important. As the gradients are propagated backward through the layers, they undergo multiple sequential computations involving the activation functions and weights of each layer. Having the intermediate activations readily available simplifies this process and ensures efficient gradient flow.\n",
    "\n",
    "By storing intermediate activations, the network can efficiently compute gradients, update parameters, and propagate information backward through the network during the backward pass. This enables effective learning and optimization of the neural network during the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. What is the downside of having activations with a standard deviation too far away from 1?**\n",
    "\n",
    "Having activations with a standard deviation too far away from 1 in a neural network can lead to several issues:\n",
    "\n",
    "1. Vanishing/Exploding Gradients: When the standard deviation of the activations is too large, it can cause the gradients to explode during backpropagation. This means that the gradient values become extremely large, which can result in unstable training and cause the model parameters to update in large and unpredictable steps. On the other hand, when the standard deviation is too small, it can lead to vanishing gradients, where the gradient values become extremely small and the network struggles to learn effectively.\n",
    "\n",
    "2. Slow Convergence: If the activations have a standard deviation significantly different from 1, it can slow down the convergence of the network during training. Activations that are too large or too small can affect the learning dynamics and make it difficult for the model to find an optimal solution. The learning process may require more iterations or epochs to reach satisfactory performance.\n",
    "\n",
    "3. Saturation of Activation Functions: Activation functions like sigmoid or tanh tend to saturate when the activations are too far away from 0. Saturation refers to the flattening of the function's slope, which leads to the gradients becoming close to zero. When the gradients are close to zero, the network's ability to learn is severely hindered, as the parameter updates become minimal and the network becomes unresponsive to further training.\n",
    "\n",
    "4. Unstable Learning Dynamics: Activations with a large standard deviation can lead to unstable learning dynamics in the network. This instability can manifest as oscillating loss values, erratic convergence behavior, or difficulty in finding a consistent and optimal solution. The network may struggle to generalize well to unseen data and exhibit poor performance.\n",
    "\n",
    "To address these issues, techniques like weight initialization methods (e.g., Xavier or He initialization) and normalization techniques (e.g., batch normalization) are commonly used. These techniques aim to initialize the network's weights and normalize the activations to maintain a more stable learning process and ensure activations with an appropriate standard deviation, closer to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. How can weight initialization help avoid this problem?**\n",
    "\n",
    "Weight initialization plays a crucial role in avoiding the problem of activations with standard deviation too far away from 1 and addressing issues like vanishing/exploding gradients. Properly initialized weights can help stabilize the learning process and ensure effective training. Here are a few ways weight initialization can help:\n",
    "\n",
    "1. Avoiding Vanishing/Exploding Gradients: Initializing the weights in a way that preserves the variance of activations helps prevent the problem of vanishing/exploding gradients. If the weights are initialized too small, the gradients can vanish as they propagate backward through the layers. On the other hand, if the weights are initialized too large, the gradients can explode. Techniques like Xavier initialization (also known as Glorot initialization) and He initialization take into account the size of the input and output dimensions of the layer to appropriately scale the weights, reducing the chances of vanishing or exploding gradients.\n",
    "\n",
    "2. Promoting Gradient Flow: Proper weight initialization can facilitate the flow of gradients through the network. By initializing the weights in a way that prevents activations from being too large or too small, the gradients can propagate smoothly during backpropagation. This helps in more efficient and stable learning, enabling the network to converge faster.\n",
    "\n",
    "3. Preserving Activation Variances: Weight initialization methods aim to preserve the variance of activations across layers during forward propagation. By maintaining consistent activation variances, the network can avoid issues like the diminishing or amplification of signal strength as it progresses through the layers. This helps in maintaining stable gradients and promoting better learning dynamics.\n",
    "\n",
    "4. Enabling Effective Learning Rates: Appropriate weight initialization can help determine a suitable learning rate for the network. When the weights are initialized too large, it may require a smaller learning rate to prevent overshooting during training. Conversely, when the weights are initialized too small, a larger learning rate may be necessary to overcome slow convergence. By initializing the weights correctly, the network can be trained more effectively with an optimal learning rate.\n",
    "\n",
    "It is worth noting that the choice of weight initialization method depends on the specific neural network architecture and the activation functions used. Different weight initialization techniques, such as Xavier initialization, He initialization, or custom initialization schemes, can be applied based on the requirements of the network and the nature of the activation functions involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

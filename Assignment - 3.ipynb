{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment - 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?**\n",
    "\n",
    "No, it is not recommended to initialize all weights to the same value, even if they are initialized using He initialization. When all the weights are initialized to the same value, the neurons in the network will produce the same output during forward propagation. This means that the gradients during backpropagation will also be the same for all the neurons in the layer, leading to symmetrical updates that do not change the network's behavior.\n",
    "\n",
    "To avoid this problem, it is recommended to initialize the weights to random values that follow a normal distribution with mean zero and a suitable variance, such as the He initialization. This will help break the symmetry and ensure that the neurons in the layer learn different features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Is it OK to initialize the bias terms to 0?**\n",
    "\n",
    "Yes, it is generally considered safe to initialize the bias terms to 0 in most cases. This is because the bias terms only add a constant offset to the input of the activation function, and the weights are responsible for adjusting the importance of the input features. As a result, initializing the bias terms to 0 will not affect the initial performance of the network significantly, and the network will learn the appropriate bias values during the training process. \n",
    "\n",
    "However, in some cases, initializing the bias terms to non-zero values may help the network to converge faster or achieve better performance, especially when the data is unbalanced or the output classes are highly imbalanced. In such cases, it may be useful to initialize the bias terms to non-zero values, such as the mean or median of the target variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Name three advantages of the SELU activation function over ReLU.**\n",
    "\n",
    "The three advantages of the SELU (Scaled Exponential Linear Unit) activation function over ReLU (Rectified Linear Unit) are:\n",
    "\n",
    "1. The SELU activation function is self-normalizing, meaning that it can automatically normalize the output of each neuron. This leads to better training stability, faster convergence, and improved generalization performance.\n",
    "\n",
    "2. The SELU activation function can produce negative outputs, which can help capture more complex and diverse patterns in the data. This can lead to better representation learning and higher model capacity.\n",
    "\n",
    "3. The SELU activation function can produce outputs with zero mean and unit variance, which can reduce the risk of vanishing or exploding gradients during backpropagation. This can enable deeper networks to be trained more effectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
    "\n",
    "The choice of activation function in a neural network depends on various factors, such as the nature of the problem being solved, the architecture of the network, and the optimization algorithm used. Here are some general guidelines for when to use some popular activation functions:\n",
    "\n",
    "1. SELU (Scaled Exponential Linear Unit): SELU is a good choice for deep neural networks with many layers. It helps to prevent the vanishing gradient problem and ensures that the mean and variance of the outputs of each layer remain constant over time.\n",
    "\n",
    "2. Leaky ReLU: Leaky ReLU is a good choice when you want to avoid the \"dying ReLU\" problem, where a ReLU neuron can become \"stuck\" at zero and never activate again. Leaky ReLU can also help to speed up convergence during training.\n",
    "\n",
    "3. ReLU (Rectified Linear Unit): ReLU is a popular choice for hidden layers in neural networks because it is computationally efficient and has been shown to work well in practice. However, it can suffer from the \"dying ReLU\" problem and is not always suitable for very deep networks.\n",
    "\n",
    "4. Tanh (Hyperbolic Tangent): Tanh is often used in the output layer of neural networks when the target values are in the range [-1, 1]. It is also used in some cases for hidden layers, but it can suffer from the vanishing gradient problem.\n",
    "\n",
    "5. Logistic (Sigmoid): Logistic activation function is used in binary classification problems where the output of the model is a probability of the positive class. It is also used in some cases for hidden layers, but it can suffer from the vanishing gradient problem.\n",
    "\n",
    "6. Softmax: Softmax is used in the output layer of neural networks when the problem involves multiclass classification. It ensures that the outputs sum up to 1, which can be interpreted as probabilities of each class.\n",
    "\n",
    "It is important to note that these are just general guidelines, and the best choice of activation function for a given problem depends on many factors and may require some experimentation.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?**\n",
    "\n",
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer may result in the optimizer becoming too slow to converge. This is because the momentum term in the optimizer accumulates too much of the past gradients, causing the optimizer to overshoot the optimal weights and take longer to converge. Additionally, setting the momentum too high can lead to oscillations or divergence in the training process. Therefore, it is recommended to keep the momentum hyperparameter between 0.9 and 0.99 for most practical applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Name three ways you can produce a sparse model.**\n",
    "\n",
    "Here are three ways to produce a sparse model:\n",
    "\n",
    "1. L1 regularization: By adding an L1 penalty term to the cost function, the optimizer is encouraged to set many of the weight values to zero, leading to a sparse model.\n",
    "\n",
    "2. Dropout: During training, random neurons are dropped out or ignored, forcing the network to rely on other neurons for making predictions. This leads to a sparse representation of the input.\n",
    "\n",
    "3. Quantization: By reducing the precision of the weights, the model can be made sparse. For example, instead of using 32-bit floating-point values, the model can use 8-bit integers, effectively making many of the weights zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?**\n",
    "\n",
    "Yes, dropout can slow down training because it forces neurons to work more independently, which may increase the number of iterations needed to converge. However, this can be partially mitigated by increasing the learning rate. Dropout does not generally slow down inference, as it is only used during training. \n",
    "\n",
    "MC Dropout (Monte Carlo Dropout) can slow down inference, as it involves making multiple predictions with dropout at test time in order to estimate the model's predictive uncertainty. However, this is only necessary if uncertainty estimates are needed, and in many cases, a single prediction without dropout may be sufficient for practical purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Practice training a deep neural network on the CIFAR10 image dataset:**\n",
    "\n",
    "**a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dd506df1d88dd73352eb3e09472b4c7cc578a4982bd023ad748be4511128f4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
